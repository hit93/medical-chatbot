
llm_model = 'meta-llama/Meta-Llama-3-8B-Instruct'
temperature = 0.7
max_tokens = 1024
top_k=2